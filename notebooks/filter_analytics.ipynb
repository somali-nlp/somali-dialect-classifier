{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Analytics: Historical Analysis\n",
    "\n",
    "This notebook demonstrates historical filter metrics analysis using Parquet data.\n",
    "\n",
    "**Features:**\n",
    "- Load Parquet data with partitioning\n",
    "- Visualize filter trends over time\n",
    "- Compare filter effectiveness across sources\n",
    "- Detect anomalies in filter behavior\n",
    "- Export insights and recommendations\n",
    "\n",
    "**Requirements:**\n",
    "```bash\n",
    "pip install pandas pyarrow duckdb matplotlib seaborn plotly\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Paths\n",
    "PARQUET_PATH = Path(\"../data/warehouse/filter_history.parquet\")\n",
    "\n",
    "print(f\"Notebook initialized at {datetime.now()}\")\n",
    "print(f\"Parquet path: {PARQUET_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Parquet data\n",
    "print(\"Loading Parquet data...\")\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "\n",
    "print(f\"âœ… Loaded {len(df):,} filter records\")\n",
    "print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"   Sources: {df['source'].nunique()} unique\")\n",
    "print(f\"   Filter types: {df['filter_type'].nunique()} unique\")\n",
    "print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and summary\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Incremental vs Full Processing Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how incremental processing works\n",
    "print(\"Incremental Processing Benefits:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count unique run_ids\n",
    "unique_runs = df['run_id'].nunique()\n",
    "total_records = len(df)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Stats:\")\n",
    "print(f\"   Unique runs: {unique_runs:,}\")\n",
    "print(f\"   Total filter records: {total_records:,}\")\n",
    "print(f\"   Avg records per run: {total_records / unique_runs:.1f}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Incremental Mode Advantages:\")\n",
    "print(f\"   - Skip {unique_runs:,} already-processed run_ids\")\n",
    "print(f\"   - Process only new metrics files\")\n",
    "print(f\"   - Merge results with existing Parquet\")\n",
    "print(f\"   - No data duplication (dedupe on run_id + filter_type)\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Storage Efficiency:\")\n",
    "parquet_size = sum(f.stat().st_size for f in PARQUET_PATH.rglob(\"*.parquet\"))\n",
    "print(f\"   Parquet size: {parquet_size / 1024**2:.2f} MB\")\n",
    "print(f\"   Partitions: source={df['source'].nunique()}, month={df['month'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization: Filter Trends Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series: Filter counts over time by source\n",
    "print(\"Creating time series visualization...\")\n",
    "\n",
    "# Aggregate by date and filter type\n",
    "daily_trends = df.groupby([pd.Grouper(key='date', freq='D'), 'filter_label'])['records_filtered'].sum().reset_index()\n",
    "\n",
    "# Get top 5 filters by total volume\n",
    "top_filters = df.groupby('filter_label')['records_filtered'].sum().nlargest(5).index\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "for filter_label in top_filters:\n",
    "    subset = daily_trends[daily_trends['filter_label'] == filter_label]\n",
    "    plt.plot(subset['date'], subset['records_filtered'], label=filter_label, marker='o', linewidth=2)\n",
    "\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Records Filtered', fontsize=12)\n",
    "plt.title('Filter Trends Over Time (Top 5 Filters by Volume)', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Time series chart created for {len(top_filters)} top filters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization: Filter Rate Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Filter rates by source and filter type\n",
    "print(\"Creating filter rate heatmap...\")\n",
    "\n",
    "# Calculate average filter rate by source and filter type\n",
    "heatmap_data = df.groupby(['source', 'filter_label'])['filter_rate'].mean().unstack(fill_value=0)\n",
    "\n",
    "# Convert to percentages\n",
    "heatmap_data = heatmap_data * 100\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'Filter Rate (%)'},\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('Average Filter Rates by Source and Filter Type (%)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Filter Type', fontsize=12)\n",
    "plt.ylabel('Source', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Heatmap created for {len(heatmap_data)} sources x {len(heatmap_data.columns)} filters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization: Top Filters by Total Rejections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Top 10 filters by total rejections\n",
    "print(\"Creating top filters bar chart...\")\n",
    "\n",
    "top_10_filters = df.groupby('filter_label')['records_filtered'].sum().nlargest(10).sort_values()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_10_filters.plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Total Records Filtered', fontsize=12)\n",
    "plt.ylabel('Filter Type', fontsize=12)\n",
    "plt.title('Top 10 Filters by Total Rejections', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(top_10_filters.values):\n",
    "    plt.text(v, i, f' {v:,.0f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Bar chart created showing top {len(top_10_filters)} filters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DuckDB SQL Query Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DuckDB queries\n",
    "print(\"Running DuckDB SQL queries...\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Query 1: Weekly aggregation\n",
    "query1 = f\"\"\"\n",
    "SELECT\n",
    "    DATE_TRUNC('week', timestamp) AS week,\n",
    "    source,\n",
    "    SUM(records_filtered) AS total_filtered,\n",
    "    COUNT(DISTINCT filter_type) AS filter_types_used\n",
    "FROM read_parquet('{PARQUET_PATH}/**/*.parquet')\n",
    "GROUP BY week, source\n",
    "ORDER BY week DESC, total_filtered DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result1 = con.execute(query1).df()\n",
    "print(\"\\nðŸ“Š Query 1: Weekly Filter Summary\")\n",
    "print(result1.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Filter category breakdown\n",
    "query2 = f\"\"\"\n",
    "SELECT\n",
    "    filter_category,\n",
    "    COUNT(DISTINCT filter_type) AS filter_count,\n",
    "    SUM(records_filtered) AS total_filtered,\n",
    "    AVG(filter_rate) * 100 AS avg_rate_pct\n",
    "FROM read_parquet('{PARQUET_PATH}/**/*.parquet')\n",
    "GROUP BY filter_category\n",
    "ORDER BY total_filtered DESC\n",
    "\"\"\"\n",
    "\n",
    "result2 = con.execute(query2).df()\n",
    "print(\"\\nðŸ“Š Query 2: Filter Category Breakdown\")\n",
    "print(result2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Source quality comparison\n",
    "query3 = f\"\"\"\n",
    "WITH SourceStats AS (\n",
    "    SELECT DISTINCT\n",
    "        run_id,\n",
    "        source,\n",
    "        records_received,\n",
    "        records_passed\n",
    "    FROM read_parquet('{PARQUET_PATH}/**/*.parquet')\n",
    ")\n",
    "SELECT\n",
    "    source,\n",
    "    COUNT(*) AS total_runs,\n",
    "    SUM(records_received) AS total_received,\n",
    "    SUM(records_passed) AS total_passed,\n",
    "    AVG(records_passed::FLOAT / NULLIF(records_received, 0)) * 100 AS avg_pass_rate_pct\n",
    "FROM SourceStats\n",
    "GROUP BY source\n",
    "ORDER BY total_received DESC\n",
    "\"\"\"\n",
    "\n",
    "result3 = con.execute(query3).df()\n",
    "print(\"\\nðŸ“Š Query 3: Source Quality Comparison\")\n",
    "print(result3.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights\n",
    "print(\"Generating insights...\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"  KEY INSIGHTS FROM FILTER ANALYTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Insight 1: Most impactful filter\n",
    "top_filter = df.groupby('filter_label')['records_filtered'].sum().idxmax()\n",
    "top_filter_count = df.groupby('filter_label')['records_filtered'].sum().max()\n",
    "print(f\"\\n1ï¸âƒ£ Most Impactful Filter:\")\n",
    "print(f\"   {top_filter}\")\n",
    "print(f\"   Total rejections: {top_filter_count:,}\")\n",
    "\n",
    "# Insight 2: Highest quality source\n",
    "source_quality = df.groupby('source').apply(\n",
    "    lambda x: (x['records_passed'].iloc[0] / x['records_received'].iloc[0]) * 100\n",
    ").sort_values(ascending=False)\n",
    "best_source = source_quality.idxmax()\n",
    "best_source_rate = source_quality.max()\n",
    "print(f\"\\n2ï¸âƒ£ Highest Quality Source:\")\n",
    "print(f\"   {best_source}\")\n",
    "print(f\"   Average pass rate: {best_source_rate:.2f}%\")\n",
    "\n",
    "# Insight 3: Most active filter category\n",
    "category_activity = df.groupby('filter_category')['records_filtered'].sum().sort_values(ascending=False)\n",
    "top_category = category_activity.idxmax()\n",
    "top_category_count = category_activity.max()\n",
    "print(f\"\\n3ï¸âƒ£ Most Active Filter Category:\")\n",
    "print(f\"   {top_category}\")\n",
    "print(f\"   Total rejections: {top_category_count:,}\")\n",
    "\n",
    "# Insight 4: Date range coverage\n",
    "date_range = (df['date'].max() - df['date'].min()).days\n",
    "print(f\"\\n4ï¸âƒ£ Historical Coverage:\")\n",
    "print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"   Total days: {date_range}\")\n",
    "print(f\"   Data density: {len(df) / date_range:.1f} records/day\")\n",
    "\n",
    "# Insight 5: Filter diversity\n",
    "filters_per_source = df.groupby('source')['filter_type'].nunique()\n",
    "print(f\"\\n5ï¸âƒ£ Filter Diversity by Source:\")\n",
    "for source, count in filters_per_source.items():\n",
    "    print(f\"   {source}: {count} unique filters\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendations\n",
    "print(\"\\nðŸ“‹ RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Filter Optimization:\")\n",
    "print(\"   - Review top 3 filters for potential relaxation\")\n",
    "print(\"   - Consider if rejection thresholds are too strict\")\n",
    "print(\"   - Monitor filter rate trends for sudden changes\")\n",
    "\n",
    "print(\"\\n2. Source Quality:\")\n",
    "print(\"   - Investigate low-quality sources (high rejection rates)\")\n",
    "print(\"   - Share best practices from high-quality sources\")\n",
    "print(\"   - Consider source-specific filter tuning\")\n",
    "\n",
    "print(\"\\n3. Data Collection:\")\n",
    "print(\"   - Continue incremental Parquet exports after each pipeline run\")\n",
    "print(\"   - Set up automated weekly trend reports\")\n",
    "print(\"   - Archive old metrics JSON to reduce storage\")\n",
    "\n",
    "print(\"\\n4. Monitoring:\")\n",
    "print(\"   - Set up alerts for anomalous filter rates (>2Ïƒ deviation)\")\n",
    "print(\"   - Track filter effectiveness over time\")\n",
    "print(\"   - Review new filter types quarterly\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV\n",
    "output_dir = Path(\"../data/reports\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Summary 1: Filter effectiveness\n",
    "filter_summary = df.groupby(['filter_label', 'filter_category']).agg({\n",
    "    'records_filtered': ['sum', 'mean', 'count'],\n",
    "    'filter_rate': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "filter_summary.columns = ['_'.join(col) for col in filter_summary.columns]\n",
    "filter_summary = filter_summary.reset_index().sort_values('records_filtered_sum', ascending=False)\n",
    "\n",
    "summary_file = output_dir / \"filter_effectiveness_summary.csv\"\n",
    "filter_summary.to_csv(summary_file, index=False)\n",
    "print(f\"âœ… Exported filter effectiveness summary to: {summary_file}\")\n",
    "\n",
    "# Summary 2: Source quality\n",
    "source_summary = df.groupby('source').agg({\n",
    "    'run_id': 'nunique',\n",
    "    'records_received': 'sum',\n",
    "    'records_passed': 'sum',\n",
    "    'records_filtered': 'sum'\n",
    "})\n",
    "\n",
    "source_summary['pass_rate'] = (source_summary['records_passed'] / source_summary['records_received'] * 100).round(2)\n",
    "source_summary = source_summary.reset_index().sort_values('records_received', ascending=False)\n",
    "\n",
    "source_file = output_dir / \"source_quality_summary.csv\"\n",
    "source_summary.to_csv(source_file, index=False)\n",
    "print(f\"âœ… Exported source quality summary to: {source_file}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Summary exports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Loading Parquet data** with Hive-style partitioning\n",
    "2. **Incremental processing benefits** - skip already-processed run_ids\n",
    "3. **Time series visualization** - filter trends over time\n",
    "4. **Heatmap analysis** - filter rates by source and type\n",
    "5. **Top filter identification** - highest rejection counts\n",
    "6. **DuckDB SQL queries** - efficient analytical queries\n",
    "7. **Insights generation** - actionable recommendations\n",
    "8. **Data export** - summary statistics to CSV\n",
    "\n",
    "**Next Steps:**\n",
    "- Set up automated weekly exports\n",
    "- Create dashboard with real-time filter metrics\n",
    "- Implement anomaly detection alerts\n",
    "- Optimize filter thresholds based on historical data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
