# CI Metrics Anomaly Detection

## Overview

The CI pipeline automatically monitors metrics files for calculation anomalies that indicate bugs in the data processing logic. This prevents metrics calculation errors from reaching production by detecting them early in the development cycle.

## What is a Metrics Anomaly?

A metrics anomaly occurs when calculated values are logically impossible or inconsistent:

| Anomaly Type | Severity | Description |
|--------------|----------|-------------|
| `records_passed_filters > records_received` | ERROR | More records passed filters than were received (impossible) |
| `quality_pass_rate > 1.0` | ERROR | Quality pass rate exceeds 100% (impossible percentage) |
| `records_written < 0` | ERROR | Negative record count (invalid) |
| `filter_breakdown sum mismatch` | WARNING | Sum of individual filter counts doesn't match total filtered |
| `records_received=0 but records_written>0` | WARNING | Records written without records received (missing instrumentation) |

## How Detection Works

### 1. Local Detection

Run anomaly detection manually:

```bash
# Scan all metrics files in data/metrics/
python scripts/check_metrics_anomalies.py

# Use custom directories
python scripts/check_metrics_anomalies.py --metrics-dir data/metrics --output test-results/anomalies.json

# Custom warning threshold
python scripts/check_metrics_anomalies.py --warning-threshold 5

# Fail on any error-level anomalies
python scripts/check_metrics_anomalies.py --fail-on-error
```

**Exit Codes:**
- `0`: No anomalies detected
- `1`: Warning-level anomalies exceeded threshold (default: 3+)
- `2`: Error-level anomalies found (with `--fail-on-error`)

### 2. CI Pipeline Detection

The GitHub Actions workflow automatically runs anomaly detection after pytest:

**Workflow:** `.github/workflows/ci.yml`

**Trigger Conditions:**
- Runs on: `ubuntu-latest` with `python-3.9` (matrix strategy)
- Executes after: `pytest --cov` completes
- Always uploads: `test-results/anomalies.json` as artifact

**Detection Steps:**

1. **Run Anomaly Check:**
   ```bash
   python scripts/check_metrics_anomalies.py --warning-threshold 3
   ```

2. **Parse Results:**
   - Extract counts: total anomalies, errors, warnings
   - Identify affected sources
   - Set GitHub Actions output variables

3. **Fail Build on Errors:**
   - If `error_count > 0`: Build fails with error annotation
   - If `warning_count >= 3`: Build continues with warning annotation

4. **Create GitHub Issue (if threshold exceeded):**
   - Condition: `warning_count >= 3` OR `error_count > 0`
   - Uses: `actions/github-script@v7`
   - Checks for existing open issue with label `metrics-anomaly`
   - Creates new issue OR adds comment to existing issue

### 3. GitHub Issue Creation

**Issue Title:**
```
⚠️ Metrics Anomaly Alert: {count} anomalies detected
```

**Issue Labels:**
- `bug`: Indicates a defect
- `metrics-anomaly`: Filterable category
- `automated`: Auto-generated by CI
- `high-priority`: Requires immediate attention

**Issue Content:**
- Total anomaly count by severity
- Affected data sources
- Commit SHA and workflow run link
- First 10 anomaly messages (with full details in artifact)
- Action steps for developers
- Threshold configuration reference
- Related documentation links

**Duplicate Prevention:**
- Checks for existing open issues with labels `metrics-anomaly` + `automated`
- If found: Adds comment to existing issue (no new issue)
- If not found: Creates new issue

## Configuration Options

### Environment Variables (CI)

Set in `.github/workflows/ci.yml`:

```yaml
env:
  ANOMALY_WARNING_THRESHOLD: 3    # Number of warnings to trigger issue creation
  ANOMALY_FAIL_ON_ERROR: true     # Fail build if error-level anomalies found
```

### Script Arguments

**`scripts/check_metrics_anomalies.py` options:**

| Argument | Default | Description |
|----------|---------|-------------|
| `--metrics-dir` | `data/metrics` | Directory containing `*_processing.json` files |
| `--output` | `test-results/anomalies.json` | Output file for anomaly report |
| `--warning-threshold` | `3` | Number of warnings to trigger exit code 1 |
| `--fail-on-error` | `false` | Exit with code 2 if any error-level anomalies |

### Threshold Tuning

Current thresholds are based on empirical data:

| Anomaly Count | CI Action | Rationale |
|---------------|-----------|-----------|
| 0 | Pass | No issues detected |
| 1-2 warnings | Pass with annotation | Transient issues (network errors, API rate limits) |
| 3+ warnings | Create Issue | Systematic issue requiring investigation |
| 1+ errors | Fail Build + Create Issue | Critical bug blocking release |

**To adjust thresholds:**

1. Edit `.github/workflows/ci.yml` line 68:
   ```yaml
   python scripts/check_metrics_anomalies.py --warning-threshold 5  # Increase to 5
   ```

2. Edit condition line 99:
   ```yaml
   (steps.anomaly_check.outputs.warning_count >= 5 || steps.anomaly_check.outputs.error_count > 0)
   ```

## Responding to Anomaly Alerts

### When an Issue is Created

1. **Review Workflow Run:**
   - Click the workflow run link in the issue
   - Navigate to "Summary" → "Artifacts"
   - Download `metrics-anomaly-report` artifact
   - Open `test-results/anomalies.json`

2. **Analyze Anomalies:**
   ```bash
   # Pretty-print anomaly report
   cat test-results/anomalies.json | python -m json.tool

   # Filter by severity
   cat test-results/anomalies.json | jq '.anomalies[] | select(.level=="error")'

   # Group by source
   cat test-results/anomalies.json | jq '.anomalies | group_by(.source)'
   ```

3. **Identify Root Cause:**

   **Anomaly: `records_passed_filters > records_received`**
   - **Likely Cause:** Incorrect `records_received` initialization
   - **Check:** `metrics.py` `record_received()` calls
   - **Fix:** Ensure all input records are counted

   **Anomaly: `filter_breakdown sum mismatch`**
   - **Likely Cause:** Missing `record_filter_reason()` call
   - **Check:** Filter logic in `*_processor.py`
   - **Fix:** Add instrumentation for each filter decision

   **Anomaly: `quality_pass_rate > 1.0`**
   - **Likely Cause:** Division error or incorrect numerator/denominator
   - **Check:** `metrics_schema.py` quality calculation
   - **Fix:** Correct calculation formula

4. **Re-run Affected Pipeline Locally:**
   ```bash
   # Example: TikTok pipeline
   python -m somali_dialect_classifier.cli process-tiktok data/tiktok_urls.txt --max-videos 1

   # Check metrics
   cat data/metrics/*tiktok*processing.json | python -m json.tool

   # Verify anomalies resolved
   python scripts/check_metrics_anomalies.py
   ```

5. **Create Fix and Verify:**
   ```bash
   # Make code changes
   git add src/somali_dialect_classifier/utils/metrics.py

   # Commit with issue reference
   git commit -m "fix(metrics): correct records_received calculation (fixes #123)"

   # Push and verify CI passes
   git push origin feature-branch
   ```

6. **Close Issue:**
   - CI will automatically verify fix
   - If no anomalies: manually close issue with comment summarizing fix
   - If anomalies persist: CI will add new comment

### False Positives

**If anomalies are expected (e.g., test data edge cases):**

1. **Add exception logic to script:**
   ```python
   # In scripts/check_metrics_anomalies.py
   # Skip test sources
   if source.startswith("test-"):
       continue
   ```

2. **Adjust tolerance for warnings:**
   ```python
   # In check_file_for_anomalies()
   tolerance = max(1, int(records_filtered * 0.10))  # Increase to 10%
   ```

3. **Update thresholds in CI:**
   ```yaml
   python scripts/check_metrics_anomalies.py --warning-threshold 5
   ```

## Output Format

**`test-results/anomalies.json` schema:**

```json
{
  "total_files": 5,
  "total_anomalies": 2,
  "error_count": 1,
  "warning_count": 1,
  "anomalies": [
    {
      "file": "20251102_071104_tiktok-somali_d400941e_processing.json",
      "source": "tiktok-somali",
      "level": "error",
      "message": "METRICS_ANOMALY: records_passed_filters (150) > records_received (100)",
      "timestamp": "2025-11-02T07:11:04Z",
      "details": {
        "records_received": 100,
        "records_passed_filters": 150,
        "records_filtered": 50
      }
    }
  ],
  "sources_affected": ["tiktok-somali"],
  "generated_at": "2025-11-02T10:00:00Z"
}
```

**Fields:**
- `total_files`: Number of `*_processing.json` files scanned
- `total_anomalies`: Sum of errors + warnings
- `error_count`: Critical anomalies (build-failing)
- `warning_count`: Non-critical anomalies (investigation recommended)
- `anomalies[]`: Array of anomaly objects
  - `file`: Metrics filename
  - `source`: Data source name (e.g., "wikipedia-somali")
  - `level`: "error" or "warning"
  - `message`: Human-readable description
  - `timestamp`: When metrics were generated (ISO 8601)
  - `details`: Additional diagnostic information
- `sources_affected`: Unique list of affected sources
- `generated_at`: When anomaly report was generated

## Integration with Existing Workflows

### Pre-commit Hook (Optional)

Add to `.git/hooks/pre-commit`:

```bash
#!/bin/bash
# Check for metrics anomalies before committing

if [ -d "data/metrics" ]; then
    echo "Checking for metrics anomalies..."
    python scripts/check_metrics_anomalies.py --fail-on-error

    if [ $? -eq 2 ]; then
        echo "❌ COMMIT BLOCKED: Error-level metrics anomalies detected"
        echo "   Run: python scripts/check_metrics_anomalies.py"
        exit 1
    fi
fi
```

### Pipeline Orchestration

Add to `src/somali_dialect_classifier/orchestration/flows.py`:

```python
def post_processing_validation(source: str):
    """Run metrics validation after processing."""
    import subprocess

    result = subprocess.run(
        ["python", "scripts/check_metrics_anomalies.py", "--fail-on-error"],
        capture_output=True,
        text=True
    )

    if result.returncode == 2:
        raise ValueError(f"Metrics anomalies detected for {source}: {result.stdout}")
```

## Troubleshooting

### Issue: Script fails with "ModuleNotFoundError"

**Cause:** Script dependencies not installed

**Solution:**
```bash
pip install -e ".[dev]"
```

### Issue: "Metrics directory not found"

**Cause:** No `data/metrics/` directory or no `*_processing.json` files

**Solution:**
```bash
# Run a pipeline to generate metrics
python -m somali_dialect_classifier.cli process-wikipedia

# Or specify custom directory
python scripts/check_metrics_anomalies.py --metrics-dir /path/to/metrics
```

### Issue: Too many false positive warnings

**Cause:** Thresholds too sensitive for your data characteristics

**Solution:**
1. Analyze false positives: `cat test-results/anomalies.json | jq '.anomalies[] | select(.level=="warning")'`
2. Adjust tolerance in script (e.g., increase from 5% to 10%)
3. Increase warning threshold: `--warning-threshold 5`

### Issue: GitHub Issue not created in CI

**Cause:** Permissions issue or missing `github-script` action

**Solution:**
1. Check workflow permissions in `.github/workflows/ci.yml`:
   ```yaml
   permissions:
     issues: write
     contents: read
   ```
2. Verify `actions/github-script@v7` is available
3. Check GitHub Actions logs for error messages

## Related Documentation

- **Architecture:** `.claude/reports/arch/arch-filter-enhancements-20251102.md` (Enhancement #2)
- **Metrics Schema:** `docs/reference/metrics-schema.md`
- **Processing Pipelines:** `docs/howto/processing-pipelines.md`
- **CI Configuration:** `.github/workflows/ci.yml`

## Change Log

- 2025-11-02: Initial implementation of CI metrics anomaly detection
  - Created `scripts/check_metrics_anomalies.py` with 5 anomaly checks
  - Integrated into `.github/workflows/ci.yml` with GitHub Issue automation
  - Configured thresholds: 3+ warnings OR 1+ errors triggers issue
  - Added artifact upload for anomaly reports (30-day retention)
