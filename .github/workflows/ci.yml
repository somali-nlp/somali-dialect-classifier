name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  lint-and-type-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run ruff linter
        run: |
          ruff check src/ tests/

      - name: Run ruff formatter check
        run: |
          ruff format --check src/ tests/

      - name: Run mypy type checker
        run: |
          mypy src/

  test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]
        python-version: ['3.9', '3.10', '3.11']

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Generate regression test fixtures
        run: |
          echo "Generating regression test fixtures..."
          python tests/regression/fixtures/generate_test_metrics.py

      - name: Run regression tests
        run: |
          echo "Running filter telemetry regression tests..."
          pytest tests/regression/test_filter_telemetry.py -v --tb=short
        continue-on-error: false

      - name: Run tests with coverage
        run: |
          pytest --cov --cov-report=xml --cov-report=term-missing

      - name: Check for metrics anomalies
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.9'
        id: anomaly_check
        run: |
          echo "Running metrics anomaly detection..."
          python scripts/check_metrics_anomalies.py --warning-threshold 3 || echo "anomaly_exit_code=$?" >> $GITHUB_OUTPUT

          # Check if anomalies.json exists and parse it
          if [ -f test-results/anomalies.json ]; then
            ANOMALY_COUNT=$(python -c "import json; data=json.load(open('test-results/anomalies.json')); print(data['total_anomalies'])")
            ERROR_COUNT=$(python -c "import json; data=json.load(open('test-results/anomalies.json')); print(data['error_count'])")
            WARNING_COUNT=$(python -c "import json; data=json.load(open('test-results/anomalies.json')); print(data['warning_count'])")
            SOURCES=$(python -c "import json; data=json.load(open('test-results/anomalies.json')); print(','.join(data['sources_affected']))")

            echo "anomaly_count=$ANOMALY_COUNT" >> $GITHUB_OUTPUT
            echo "error_count=$ERROR_COUNT" >> $GITHUB_OUTPUT
            echo "warning_count=$WARNING_COUNT" >> $GITHUB_OUTPUT
            echo "sources_affected=$SOURCES" >> $GITHUB_OUTPUT

            if [ "$ERROR_COUNT" -gt 0 ]; then
              echo "::error::Found $ERROR_COUNT error-level metrics anomalies - see test results"
              exit 1
            elif [ "$WARNING_COUNT" -ge 3 ]; then
              echo "::warning::Found $WARNING_COUNT warning-level metrics anomalies - see test results"
            fi
          else
            echo "anomaly_count=0" >> $GITHUB_OUTPUT
            echo "error_count=0" >> $GITHUB_OUTPUT
            echo "warning_count=0" >> $GITHUB_OUTPUT
            echo "sources_affected=" >> $GITHUB_OUTPUT
          fi

      - name: Create GitHub Issue for metrics anomalies
        if: |
          matrix.os == 'ubuntu-latest' &&
          matrix.python-version == '3.9' &&
          (steps.anomaly_check.outputs.warning_count >= 3 || steps.anomaly_check.outputs.error_count > 0)
        uses: actions/github-script@v7
        env:
          ANOMALY_COUNT: ${{ steps.anomaly_check.outputs.anomaly_count }}
          ERROR_COUNT: ${{ steps.anomaly_check.outputs.error_count }}
          WARNING_COUNT: ${{ steps.anomaly_check.outputs.warning_count }}
          SOURCES: ${{ steps.anomaly_check.outputs.sources_affected }}
        with:
          script: |
            const fs = require('fs');
            const anomalyCount = process.env.ANOMALY_COUNT;
            const errorCount = process.env.ERROR_COUNT;
            const warningCount = process.env.WARNING_COUNT;
            const sources = process.env.SOURCES;

            // Load anomaly details from JSON
            let anomalyDetails = '';
            try {
              const anomalyData = JSON.parse(fs.readFileSync('test-results/anomalies.json', 'utf8'));
              anomalyDetails = anomalyData.anomalies
                .slice(0, 10)  // Show first 10 anomalies
                .map(a => `- **[${a.level.toUpperCase()}]** ${a.source}: ${a.message}`)
                .join('\n');

              if (anomalyData.anomalies.length > 10) {
                anomalyDetails += `\n\n_... and ${anomalyData.anomalies.length - 10} more anomalies (see artifact)_`;
              }
            } catch (err) {
              anomalyDetails = '_Failed to load anomaly details from test-results/anomalies.json_';
            }

            // Check if issue already exists
            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'metrics-anomaly,automated'
            });

            const issueBody = `## Metrics Calculation Anomalies Detected

            **Total Anomalies:** ${anomalyCount}
            **Error-level:** ${errorCount}
            **Warning-level:** ${warningCount}
            **Sources Affected:** ${sources || 'None'}
            **Commit:** ${context.sha.substring(0, 7)}
            **Workflow Run:** [View Details](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})

            ### Anomaly Details

            ${anomalyDetails}

            ### What This Means

            The CI pipeline detected metrics calculation anomalies. These indicate potential bugs in:
            - \`src/somali_dialect_classifier/utils/metrics.py\`
            - \`src/somali_dialect_classifier/utils/metrics_schema.py\`
            - Data source processors (e.g., \`preprocessing/*_processor.py\`)

            ### Action Required

            1. **Review Test Results:** Download the \`test-results\` artifact from the workflow run
            2. **Check Anomaly Report:** Review \`test-results/anomalies.json\` for full details
            3. **Investigate Root Cause:** Check the affected data source processors
            4. **Fix Calculation Bug:** Update metrics calculation logic as needed
            5. **Verify Fix:** Run \`python scripts/check_metrics_anomalies.py\` locally

            ### Threshold Configuration

            - **Warning Threshold:** 3+ warning-level anomalies → Issue created
            - **Error Threshold:** 1+ error-level anomalies → Build fails + Issue created

            ### Related Documentation

            - [Metrics Schema Reference](docs/reference/metrics-schema.md)
            - [Processing Pipelines Guide](docs/howto/processing-pipelines.md)

            ---
            _This issue was automatically created by the CI pipeline._
            `;

            if (existingIssues.data.length === 0) {
              // Create new issue
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `⚠️ Metrics Anomaly Alert: ${anomalyCount} anomalies detected`,
                body: issueBody,
                labels: ['bug', 'metrics-anomaly', 'automated', 'high-priority']
              });
              console.log('Created new metrics anomaly issue');
            } else {
              // Add comment to existing issue
              const existingIssue = existingIssues.data[0];
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: existingIssue.number,
                body: `### New Anomalies Detected\n\n${issueBody}`
              });
              console.log(`Added comment to existing issue #${existingIssue.number}`);
            }

      - name: Upload anomaly report
        if: always() && matrix.os == 'ubuntu-latest' && matrix.python-version == '3.9'
        uses: actions/upload-artifact@v4
        with:
          name: metrics-anomaly-report
          path: test-results/anomalies.json
          retention-days: 30

      - name: Upload coverage to Codecov
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.9'
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          fail_ci_if_error: false

  test-dashboard:
    name: Dashboard Visual Tests
    runs-on: ubuntu-latest
    needs: [lint-and-type-check]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install Python dependencies
        run: |
          pip install pandas plotly numpy

      - name: Install Playwright dependencies
        run: |
          npm ci
          npx playwright install --with-deps chromium

      - name: Build dashboard
        run: |
          chmod +x dashboard/build-site.sh
          ./dashboard/build-site.sh

      - name: Generate test metrics data
        run: |
          mkdir -p _site/data
          python << 'PYTHON_SCRIPT'
          import json
          from pathlib import Path
          from datetime import datetime

          site_data = Path("_site/data")

          metrics_dir = Path("data/metrics")
          all_metrics = []

          if metrics_dir.exists():
              for metrics_file in sorted(metrics_dir.glob("*_processing.json")):
                  try:
                      with open(metrics_file) as f:
                          data = json.load(f)

                          if "_schema_version" in data:
                              legacy_wrapper = data.get("legacy_metrics", {})
                              snapshot = legacy_wrapper.get("snapshot", {})
                              stats = legacy_wrapper.get("statistics", {})
                          else:
                              snapshot = data.get("snapshot", {})
                              stats = data.get("statistics", {})

                          perf = stats.get("throughput", {})
                          quality = stats.get("text_length_stats", {})
                          pipeline_type = snapshot.get("pipeline_type", "unknown")

                          metric_entry = {
                              "run_id": snapshot.get("run_id", ""),
                              "source": snapshot.get("source", ""),
                              "timestamp": snapshot.get("timestamp", ""),
                              "duration_seconds": snapshot.get("duration_seconds", 0),
                              "pipeline_type": pipeline_type,
                              "urls_discovered": snapshot.get("urls_discovered", 0),
                              "urls_fetched": snapshot.get("urls_fetched", 0),
                              "urls_processed": snapshot.get("urls_processed", 0),
                              "records_written": snapshot.get("records_written", 0),
                              "bytes_downloaded": snapshot.get("bytes_downloaded", 0),
                              "pipeline_metrics": {
                                  "pipeline_type": pipeline_type,
                                  "quality_pass_rate": stats.get("quality_pass_rate", 0),
                                  "deduplication_rate": stats.get("deduplication_rate", 0),
                              },
                              "performance": {
                                  "urls_per_second": perf.get("urls_per_second", 0),
                                  "bytes_per_second": perf.get("bytes_per_second", 0),
                                  "records_per_minute": perf.get("records_per_minute", 0)
                              },
                              "quality": {
                                  "min": quality.get("min", 0),
                                  "max": quality.get("max", 0),
                                  "mean": quality.get("mean", 0),
                                  "median": quality.get("median", 0),
                                  "total_chars": quality.get("total_chars", 0)
                              }
                          }

                          all_metrics.append(metric_entry)

                  except Exception as e:
                      print(f"Warning: {metrics_file}: {e}")

          output = {
              "count": len(all_metrics),
              "records": sum(m["records_written"] for m in all_metrics),
              "sources": sorted(list(set(m["source"] for m in all_metrics))),
              "pipeline_types": sorted(list(set(m["pipeline_type"] for m in all_metrics))),
              "metrics": all_metrics,
              "schema_version": "2.0"
          }

          with open("_site/data/all_metrics.json", "w") as f:
              json.dump(output, f, indent=2)

          print(f"Generated metrics: {len(all_metrics)} runs")
          PYTHON_SCRIPT

      - name: Run Playwright tests
        env:
          DASHBOARD_URL: http://localhost:8000
        run: |
          npx playwright test --project=chromium

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report
          path: test-results/
          retention-days: 30

      - name: Upload test report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-html-report
          path: test-results/html-report/
          retention-days: 30
