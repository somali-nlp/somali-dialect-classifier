name: Deploy Enhanced Dashboard to GitHub Pages

on:
  push:
    branches:
      - main
    paths:
      - 'data/metrics/**'
      - 'data/reports/**'
      - 'dashboard/**'
      - '.github/workflows/deploy-dashboard-v2.yml'
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment
concurrency:
  group: "pages"
  cancel-in-progress: true

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install streamlit pandas plotly numpy

      - name: Build dashboard site
        run: |
          # Step 1: Create site structure and copy static assets
          # - Creates _site/ and _site/data/ directories
          # - Copies index.html template
          # - Copies favicon, metrics/, and reports/ directories
          # - Does NOT generate all_metrics.json (done in next step)
          chmod +x dashboard/build-site.sh
          ./dashboard/build-site.sh

      - name: Generate metrics data
        run: |
          # Step 2: Process raw metrics and generate aggregated JSON
          # - Reads all *_processing.json files from data/metrics/
          # - Detects pipeline type and extracts appropriate metrics
          # - Supports both old (fetch_success_rate) and new pipeline-specific metric names
          # - Generates _site/data/all_metrics.json for dashboard consumption
          # - This step MUST run after build-site.sh to ensure directories exist
          python << 'PYTHON_SCRIPT'
          import json
          from pathlib import Path
          from datetime import datetime

          # Create data directory (redundant with build-site.sh, but ensures it exists)
          site_data = Path("_site/data")
          site_data.mkdir(parents=True, exist_ok=True)

          def extract_pipeline_metrics(snapshot, stats):
              """Extract metrics based on pipeline type with backward compatibility."""
              pipeline_type = snapshot.get("pipeline_type", "unknown")

              # Base metrics structure
              metrics = {
                  "pipeline_type": pipeline_type,
                  "quality_pass_rate": stats.get("quality_pass_rate", 0),
                  "deduplication_rate": stats.get("deduplication_rate", 0),
              }

              # Pipeline-specific metrics with backward compatibility
              if pipeline_type == "web_scraping":
                  # Try new names first, fall back to old generic name
                  metrics["http_success_rate"] = stats.get("http_request_success_rate",
                                                           stats.get("fetch_success_rate", 0))
                  metrics["extraction_rate"] = stats.get("content_extraction_success_rate",
                                                         stats.get("fetch_success_rate", 0))

              elif pipeline_type == "file_processing":
                  # Try new names first, fall back to old generic name
                  metrics["file_extraction_rate"] = stats.get("file_extraction_success_rate",
                                                              stats.get("fetch_success_rate", 0))
                  metrics["parsing_rate"] = stats.get("record_parsing_success_rate",
                                                      stats.get("fetch_success_rate", 0))

              elif pipeline_type == "stream_processing":
                  # Try new names first, fall back to old generic name
                  metrics["connection_success"] = stats.get("stream_connection_success_rate",
                                                           stats.get("fetch_success_rate", 0))
                  metrics["retrieval_rate"] = stats.get("record_retrieval_success_rate",
                                                        stats.get("fetch_success_rate", 0))
                  metrics["coverage_rate"] = stats.get("dataset_coverage_rate", 0)
              else:
                  # Unknown pipeline type - use generic fallback
                  metrics["success_rate"] = stats.get("fetch_success_rate", 0)

              # Keep old success_rate for backward compatibility (deprecated)
              metrics["success_rate_deprecated"] = stats.get("fetch_success_rate", 0)

              return metrics

          # Load all metrics (only processing phase for accurate success rates)
          metrics_dir = Path("data/metrics")
          all_metrics = []

          if metrics_dir.exists():
              for metrics_file in metrics_dir.glob("*_processing.json"):
                  try:
                      with open(metrics_file) as f:
                          data = json.load(f)
                          snapshot = data.get("snapshot", {})
                          stats = data.get("statistics", {})
                          perf = stats.get("throughput", {})
                          quality = stats.get("text_length_stats", {})

                          # Extract pipeline-specific metrics
                          pipeline_metrics = extract_pipeline_metrics(snapshot, stats)

                          metric_entry = {
                              "run_id": snapshot.get("run_id", ""),
                              "source": snapshot.get("source", ""),
                              "timestamp": snapshot.get("timestamp", ""),
                              "duration_seconds": snapshot.get("duration_seconds", 0),
                              "pipeline_type": snapshot.get("pipeline_type", "unknown"),

                              # Volume metrics
                              "urls_discovered": snapshot.get("urls_discovered", 0),
                              "urls_fetched": snapshot.get("urls_fetched", 0),
                              "urls_processed": snapshot.get("urls_processed", 0),
                              "files_discovered": snapshot.get("files_discovered", 0),
                              "files_processed": snapshot.get("files_processed", 0),
                              "records_fetched": snapshot.get("records_fetched", 0),
                              "records_written": snapshot.get("records_written", 0),
                              "bytes_downloaded": snapshot.get("bytes_downloaded", 0),

                              # Pipeline-specific success metrics
                              "pipeline_metrics": pipeline_metrics,

                              # Performance metrics
                              "performance": {
                                  "urls_per_second": perf.get("urls_per_second", 0),
                                  "bytes_per_second": perf.get("bytes_per_second", 0),
                                  "records_per_minute": perf.get("records_per_minute", 0)
                              },

                              # Quality metrics
                              "quality": {
                                  "min": quality.get("min", 0),
                                  "max": quality.get("max", 0),
                                  "mean": quality.get("mean", 0),
                                  "median": quality.get("median", 0),
                                  "total_chars": quality.get("total_chars", 0)
                              }
                          }

                          all_metrics.append(metric_entry)

                  except Exception as e:
                      print(f"âš ï¸  Error loading {metrics_file}: {e}")

          # Calculate aggregates
          if all_metrics:
              total_records = sum(m["records_written"] for m in all_metrics)
              sources = sorted(list(set(m["source"] for m in all_metrics)))
              pipeline_types = sorted(list(set(m["pipeline_type"] for m in all_metrics)))

              # Save all metrics
              output = {
                  "count": len(all_metrics),
                  "records": total_records,
                  "sources": sources,
                  "pipeline_types": pipeline_types,
                  "metrics": all_metrics,
                  "schema_version": "2.0",  # Indicates new pipeline-specific structure
                  "backward_compatible": True  # Indicates old fields still present
              }

              with open("_site/data/all_metrics.json", "w") as f:
                  json.dump(output, f, indent=2)

              print(f"âœ… Generated metrics: {len(all_metrics)} runs, {total_records:,} records")
              print(f"âœ… Sources: {', '.join(sources)}")
              print(f"âœ… Pipeline types: {', '.join(pipeline_types)}")
              print(f"ðŸ“Š Schema version: 2.0 (pipeline-specific metrics)")
          else:
              # No metrics found - create empty files
              print("âš ï¸  No metrics found - creating empty state")

              empty_output = {
                  "count": 0,
                  "records": 0,
                  "sources": [],
                  "pipeline_types": [],
                  "metrics": [],
                  "schema_version": "2.0",
                  "backward_compatible": True
              }

              with open("_site/data/all_metrics.json", "w") as f:
                  json.dump(empty_output, f, indent=2)

              print("âœ… Created empty metrics files")
          PYTHON_SCRIPT

      - name: Verify build
        run: |
          # Step 3: Verify all required files are present before deployment
          # - Ensures _site/data/all_metrics.json exists (critical for dashboard)
          # - Displays structure and content preview
          echo "ðŸ“¦ Build verification:"
          echo ""
          echo "ðŸ“„ Site structure:"
          ls -lh _site/
          echo ""
          echo "ðŸ“ Data directory:"
          ls -lh _site/data/
          echo ""
          if [ -f "_site/data/all_metrics.json" ]; then
            echo "âœ… all_metrics.json exists"
            echo "ðŸ“Š Metrics preview:"
            head -20 _site/data/all_metrics.json
          else
            echo "âŒ ERROR: all_metrics.json not found!"
            exit 1
          fi
          echo ""
          echo "âœ… Build verification complete"

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: '_site'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

      - name: Summary
        run: |
          echo "ðŸš€ Deployment complete!"
          echo "ðŸ“Š Dashboard URL: https://somali-nlp.github.io/somali-dialect-classifier/"
