"""
CLI entrypoint for downloading and processing BBC Somali news.

This module provides the command-line interface for:
- Scraping BBC Somali articles ethically
- Processing and cleaning news content
- Creating silver datasets
"""

import argparse
import logging
from logging.handlers import RotatingFileHandler
from pathlib import Path

from somali_dialect_classifier.preprocessing import BBCSomaliProcessor


def _setup_logging() -> None:
    """
    Set up logging for BBC pipeline.

    Uses module-specific logger to prevent cross-contamination with other pipelines.
    """
    # Console logs
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        force=True  # Reset existing handlers
    )

    # File logs (rotating) under logs/
    logs_dir = Path('logs')
    logs_dir.mkdir(exist_ok=True)

    # Create handler for BBC-specific log file
    fh = RotatingFileHandler(
        logs_dir / 'download_bbcsom.log',
        maxBytes=5_000_000,
        backupCount=3
    )
    fh.setLevel(logging.INFO)
    fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))

    # Add handler to capture all logs from processors
    # Use root logger but with module filter
    root_logger = logging.getLogger()

    # Remove any existing file handlers to prevent duplication
    root_logger.handlers = [h for h in root_logger.handlers if not isinstance(h, RotatingFileHandler)]

    # Add our file handler
    root_logger.addHandler(fh)


def main() -> None:
    """
    Main entry point for executing the BBC Somali data scraping pipeline.

    This function sets up logging, initializes the `BBCSomaliProcessor`,
    and explicitly orchestrates discover, scrape, and process steps.
    After completion, it prints the paths to the raw, staging, and processed files.
    """
    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description='Download and process BBC Somali news articles'
    )
    parser.add_argument(
        '--max-articles',
        type=int,
        default=None,
        help='Maximum number of articles to scrape (default: unlimited - scrapes all discovered articles)'
    )
    parser.add_argument(
        '--min-delay',
        type=int,
        default=3,
        help='Minimum delay between requests in seconds (default: 3)'
    )
    parser.add_argument(
        '--max-delay',
        type=int,
        default=6,
        help='Maximum delay between requests in seconds (default: 6)'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='Force reprocessing even if output files exist'
    )
    args = parser.parse_args()

    _setup_logging()

    try:
        # Initialize with user-specified parameters
        processor = BBCSomaliProcessor(
            max_articles=args.max_articles,
            delay_range=(args.min_delay, args.max_delay),
            force=args.force
        )

        article_links_file = processor.download()
        staging_file = processor.extract()
        processed_file = processor.process()

        print(f"\nâœ“ Pipeline completed successfully!")
        print(f"  Article links: {article_links_file}")
        print(f"  Scraped articles: {staging_file}")
        print(f"  Processed: {processed_file}")

    except Exception as e:
        logging.error(f"Pipeline failed: {e}")
        raise


if __name__ == "__main__":
    main()
